{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_ds3_gan_complete.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3cK7pyDlUd53X572La4Y0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mneunhoe/ds3_gan/blob/main/01_ds3_gan_complete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVrs0WARyxPP"
      },
      "source": [
        "# $\\text{DS}^3$: Generative Adversarial Nets\n",
        "(Marcel Neunhoeffer, LMU Munich)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_hlE-QEzvyH"
      },
      "source": [
        "This is the first workbook for the Data Science Summer School course \"Generative Adversarial Nets\". \n",
        "\n",
        "You can find the slides and the following workbooks for this course at: https://github.com/mneunhoe/ds3_gan\n",
        "\n",
        "\n",
        "In this workbook we will implement a simple Generative Adversarial Net from scratch. Along the way we will learn a lot about basic and advanced design choices of GANs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmoQAzu61Lhn"
      },
      "source": [
        "Before we get started, we install and load some helpful packages to our R environment. Installing `torch` can take some time (around 6 minutes for me)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s_AizUt1KQU"
      },
      "source": [
        "p_needed <- c(\"viridis\", \"skimr\", \"torch\", \"RGAN\")\n",
        "packages <- rownames(installed.packages())\n",
        "p_to_install <- p_needed[!(p_needed %in% packages)]\n",
        "if (length(p_to_install) > 0) {\n",
        "  install.packages(p_to_install)\n",
        "}\n",
        "sapply(p_needed, require, character.only = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33FnQ8-QzeJ_"
      },
      "source": [
        "## A GAN from scratch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOHcyGBKClg1"
      },
      "source": [
        "### Setting up some toydata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phEhLlt1xDLp"
      },
      "source": [
        "First, we generate some simple two dimensional toy data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6MTeFvv18U5"
      },
      "source": [
        "# We setup some toy data where we know the true data generating process.\n",
        "\n",
        "# Using a seed we will all get the same \"random\" data\n",
        "set.seed(220803)\n",
        "\n",
        "# The number of observations\n",
        "n <- 1000\n",
        "\n",
        "# x is drawn from a standard normal distribution\n",
        "x <- c(stats::rnorm(n))\n",
        "\n",
        "# y is x^2 plus some random noise from a normal distribution with sd = 0.3\n",
        "y <- c(stats::rnorm(n, x^2, 0.3))\n",
        "\n",
        "# x and y put together is our toydata\n",
        "toydata <- cbind(x, y)\n",
        "\n",
        "# We later need the data as a torch tensor\n",
        "toydata_tensor <- torch::torch_tensor(toydata)\n",
        "\n",
        "# Get an overview of the dataset\n",
        "\n",
        "# With a skim summary\n",
        "skimr::skim(toydata)\n",
        "\n",
        "# With a simple scatterplot\n",
        "plot(toydata[,1], toydata[,2],\n",
        "      main = \"Toydata\", xlab = \"x\", ylab = \"y\", las = 1,\n",
        "      col = viridis::viridis(1, 0.6), \n",
        "      pch = 19, bty = \"n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNfpDeGXUq2z"
      },
      "source": [
        "### Setting up the neural networks\n",
        "As we have seen, a GAN consists of multiple neural networks: \n",
        "- A Generator that gets random noise as input and produces fake samples as output.\n",
        "- A Discriminator that gets real and fake samples as input and tries to classify which examples are real and which are fake.\n",
        "\n",
        "In this example we want to generate fake samples from our toydata. Before we can observe the adversarial game between Generator and Discriminator we need to set up the neural networks.\n",
        "\n",
        "Here we use the `torch` library to set up the networks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Generator <- torch::nn_module(\n",
        "  initialize = function(noise_dim,\n",
        "                        data_dim,\n",
        "                        hidden_units = list(128, 128),\n",
        "                        dropout_rate = 0\n",
        "                        ) {\n",
        "                        # Initialize an empty nn_sequential module\n",
        "                        self$seq <- torch::nn_sequential()\n",
        "\n",
        "                        # For the hidden layers we need to keep track of our input and output dimensions. The first input will be our noise vector, therefore, it will be noise_dim\n",
        "                        dim <- noise_dim\n",
        "\n",
        "                        # i will be a simple counter to keep track of our network depth\n",
        "                        i <- 1\n",
        "\n",
        "                        # Now we loop over the list of hidden units and add the hidden layers to the nn_sequential module\n",
        "                        for (neurons in hidden_units) {\n",
        "                          # First, we add a ResidualBlock of the respective size.\n",
        "                          self$seq$add_module(module =  torch::nn_linear(dim, neurons),\n",
        "                                              name = paste0(\"Linear_\", i))\n",
        "\n",
        "                          # Add a ReLU activation\n",
        "                          self$seq$add_module(module = torch::nn_relu(),\n",
        "                                              name = paste0(\"Activation_\", i))\n",
        "                          # And then a Dropout layer.\n",
        "                          self$seq$add_module(module = torch::nn_dropout(dropout_rate),\n",
        "                                              name = paste0(\"Dropout_\", i))\n",
        "                          # Now we update our dim for the next hidden layer.\n",
        "                          dim <- neurons\n",
        "                          # Update the counter\n",
        "                          i <- i + 1\n",
        "                        }\n",
        "                        # Finally, we add the output layer. The output dimension must be the same as our data dimension (data_dim).\n",
        "                        self$seq$add_module(module = torch::nn_linear(dim, data_dim),\n",
        "                                            name = \"Output\")\n",
        "                        },\n",
        "forward = function(input) {\n",
        "  input <- self$seq(input)\n",
        "  input\n",
        "}\n",
        "  )"
      ],
      "metadata": {
        "id": "i_tiv4_ggJ0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_net <- Generator(noise_dim = 2, data_dim = 2, hidden_units = list(128, 128))\n",
        "g_net"
      ],
      "metadata": {
        "id": "88sg13eUgdHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne0xPB_HVOUZ"
      },
      "source": [
        "Discriminator <- torch::nn_module(\n",
        "  initialize = function(data_dim,\n",
        "                        hidden_units = list(128, 128),\n",
        "                        dropout_rate = 0,\n",
        "                        sigmoid = TRUE) {\n",
        "    # Initialize an empty nn_sequential module\n",
        "    self$seq <- torch::nn_sequential()\n",
        "\n",
        "    # For the hidden layers we need to keep track of our input and output dimensions. The first input will be our real and fake data examples, therefore, it will be data_dim\n",
        "    dim <- data_dim\n",
        "\n",
        "    # i will be a simple counter to keep track of our network depth\n",
        "    i <- 1\n",
        "\n",
        "    # Now we loop over the list of hidden units and add the hidden layers to the nn_sequential module\n",
        "    for (neurons in hidden_units) {\n",
        "      # We start with a fully connected linear layer\n",
        "      self$seq$add_module(module = torch::nn_linear(dim, neurons),\n",
        "                          name = paste0(\"Linear_\", i))\n",
        "      # Add a ReLU activation\n",
        "      self$seq$add_module(module = torch::nn_relu(),\n",
        "                          name = paste0(\"Activation_\", i))\n",
        "      # And a Dropout layer\n",
        "      self$seq$add_module(module = torch::nn_dropout(dropout_rate),\n",
        "                          name = paste0(\"Dropout_\", i))\n",
        "      # Update the input dimension to the next layer\n",
        "      dim <- neurons\n",
        "      # Update the counter\n",
        "      i <- i + 1\n",
        "    }\n",
        "    # Add an output layer to the net. Since it will be one score for each example we only need a dimension of 1.\n",
        "    self$seq$add_module(module = torch::nn_linear(dim, 1),\n",
        "                        name = \"Output\")\n",
        "    if (sigmoid) {\n",
        "      self$seq$add_module(module = torch::nn_sigmoid(),\n",
        "                          name = \"Sigmoid_Output\")\n",
        "    }\n",
        "\n",
        "  },\n",
        "  forward = function(input) {\n",
        "    data <- self$seq(input)\n",
        "    data\n",
        "  }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_net <- Discriminator(data_dim = 2, hidden_units = list(128, 128))\n",
        "d_net"
      ],
      "metadata": {
        "id": "zUIMdrGqhbOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up optimizers\n",
        "\n",
        "To train the neural networks we need to define optimizers for both."
      ],
      "metadata": {
        "id": "PvECRXzrh-pT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_optim <- torch::optim_adam(g_net$parameters, lr = 0.0001)\n",
        "\n",
        "d_optim <- torch::optim_adam(d_net$parameters, lr = 0.0001)"
      ],
      "metadata": {
        "id": "njUye4HOiIg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Walking through one update step"
      ],
      "metadata": {
        "id": "ELcDv9aGir3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the Discriminator\n",
        "\n",
        "# Get a batch of real data\n",
        "real_data <- toydata_tensor[sample(nrow(toydata_tensor), size = 64)]\n",
        "\n",
        "# Sample new noise\n",
        "z <- torch::torch_randn(c(64, 2))\n",
        "\n",
        "# Generate a batch of fake data\n",
        "fake_data <- torch::with_no_grad(g_net(input = z))\n",
        "\n",
        "# Calculate the discriminator scores for real and fake data\n",
        "dis_real <- d_net(real_data)\n",
        "dis_fake <- d_net(fake_data)\n",
        "\n",
        "# Calculate the loss\n",
        "d_loss <- torch::torch_log(dis_real) + torch::torch_log(1 - dis_fake)\n",
        "d_loss <- -d_loss$mean()\n",
        "\n",
        "# Take one update step for the parameters of the Discriminator\n",
        "d_optim$zero_grad()\n",
        "d_loss$backward()\n",
        "d_optim$step()\n",
        "\n",
        "# Update the Generator  \n",
        "\n",
        "# Sample new noise\n",
        "z <- torch::torch_randn(c(64, 2))\n",
        "\n",
        "# Generate a batch of fake data\n",
        "fake_data <- g_net(z)\n",
        "\n",
        "# Calculate the discriminator score for fake data\n",
        "dis_fake <- d_net(fake_data)\n",
        "\n",
        "# Calculate the loss\n",
        "g_loss <- torch::torch_log(1 - dis_fake)\n",
        "g_loss <- g_loss$mean()\n",
        "\n",
        "# Take one update step for the parameters of the Generator\n",
        "g_optim$zero_grad()\n",
        "g_loss$backward()\n",
        "g_optim$step()\n"
      ],
      "metadata": {
        "id": "6KX5JbReirjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A look at the synthetic data\n",
        "We can use the Generator to produce synthetic data and look at it. After just one training step it will not yet look anything like the toydata that we want to copy. We will use the same `fixed_z` later on to track training."
      ],
      "metadata": {
        "id": "fCpEFhg9odNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_z <- torch::torch_randn(c(1000, 2))\n",
        "\n",
        "synthetic_data <- torch::as_array(g_net(fixed_z))\n",
        "\n",
        "# With a simple scatterplot\n",
        "plot(synthetic_data[,1], synthetic_data[,2],\n",
        "      main = \"Synthetic Data\", xlab = \"x\", ylab = \"y\", las = 1,\n",
        "      col = viridis::viridis(2, 0.6)[2], \n",
        "      pch = 19, bty = \"n\")\n"
      ],
      "metadata": {
        "id": "psx4b_Y8lqxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Training loop\n",
        "\n",
        "Now we just need to repeat this many times (here 3000 times). After every 100th step we also look at the progress of our training."
      ],
      "metadata": {
        "id": "9claU5FjoVyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Objects to track the losses\n",
        "d_losses <- NULL\n",
        "g_losses <- NULL"
      ],
      "metadata": {
        "id": "3cDbTkpJwy52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for(i in 1:3000){\n",
        "# Update the Discriminator\n",
        "real_data <- toydata_tensor[sample(nrow(toydata_tensor), size = 64)]\n",
        "z <- torch::torch_randn(c(64, 2))\n",
        "\n",
        "fake_data <- torch::with_no_grad(g_net(input = z))\n",
        "\n",
        "dis_real <- d_net(real_data)\n",
        "dis_fake <- d_net(fake_data)\n",
        "\n",
        "\n",
        "d_loss <- torch::torch_log(dis_real) + torch::torch_log(1 - dis_fake)\n",
        "d_loss <- -d_loss$mean()\n",
        "\n",
        "d_losses <- c(d_losses, torch::as_array(d_loss$detach()))\n",
        "\n",
        "d_optim$zero_grad()\n",
        "d_loss$backward()\n",
        "d_optim$step()\n",
        "\n",
        "# Update the Generator  \n",
        "z <- torch::torch_randn(c(64, 2))\n",
        "fake_data <- g_net(z)\n",
        "\n",
        "dis_fake <- d_net(fake_data)\n",
        "\n",
        "g_loss <- torch::torch_log(1 - dis_fake)\n",
        "g_loss <- g_loss$mean()\n",
        "\n",
        "g_losses <- c(g_losses, torch::as_array(g_loss$detach()))\n",
        "\n",
        "g_optim$zero_grad()\n",
        "g_loss$backward()\n",
        "g_optim$step()\n",
        "\n",
        "if(i %% 100 == 0) {\n",
        "synthetic_data <- torch::as_array(g_net(fixed_z))\n",
        "\n",
        "plot(synthetic_data[,1], synthetic_data[,2],\n",
        "      main = paste0(\"Synthetic Data after step \", i), xlab = \"x\", ylab = \"y\", las = 1,\n",
        "      col = viridis::viridis(2, 0.6)[2], \n",
        "      pch = 19, bty = \"n\")\n",
        "}\n",
        "}"
      ],
      "metadata": {
        "id": "zY_lPz9QnfZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot(toydata[,1], toydata[,2],\n",
        "      main = \"Real and Synthetic data\", xlab = \"x\", ylab = \"y\", las = 1,\n",
        "      col = viridis::viridis(2, 0.6)[1], \n",
        "      pch = 19, bty = \"n\")\n",
        "points(synthetic_data[,1], synthetic_data[,2],\n",
        "      col = viridis::viridis(2, 0.6)[2], pch = 19)"
      ],
      "metadata": {
        "id": "xSiYiXDTrND4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "par(mfrow = c(1, 2))\n",
        "plot(1:length(d_losses), d_losses, type = \"l\", lwd = 2, col = viridis::viridis(3)[1], main = \"Discriminator Loss\", xlab = \"Step\", ylab = \"Loss\", bty = \"n\", las = 1)\n",
        "plot(1:length(d_losses), g_losses, type = \"l\", lwd = 2, col = viridis::viridis(3)[1], main = \"Generator Loss\", xlab = \"Step\", ylab = \"Loss\", bty = \"n\", las = 1)\n"
      ],
      "metadata": {
        "id": "MVzValZ7vEuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the `RGAN` package \n",
        "\n",
        "To make our lives a bit easier we can use the `RGAN` package. This will also make it easy to customize GANs and experiment with different choices that we will look at in the next part of the course."
      ],
      "metadata": {
        "id": "3gLFp9GByqmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch::torch_manual_seed(20220803)\n",
        "\n",
        "g_net <- RGAN::Generator(noise_dim = 2, data_dim = 2, hidden_units = list(128, 128), dropout_rate = 0)\n",
        "\n",
        "d_net <- RGAN::Discriminator(data_dim = 2, hidden_units = list(128, 128), dropout_rate = 0, sigmoid = TRUE)\n",
        "\n",
        "trained_gan <- RGAN::gan_trainer(data = toydata, noise_dim = 2, generator = g_net, discriminator = d_net, plot_progress = TRUE, plot_interval = 100)\n"
      ],
      "metadata": {
        "id": "5Vstjv8SzMff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use our `trained_gan` to sample synthetic data."
      ],
      "metadata": {
        "id": "ZH0WYmHl1WrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synthetic_data <- RGAN::sample_synthetic_data(trained_gan)\n",
        "\n",
        "plot(toydata[,1], toydata[,2],\n",
        "      main = \"Real and Synthetic data\", xlab = \"x\", ylab = \"y\", las = 1,\n",
        "      col = viridis::viridis(2, 0.6)[1], \n",
        "      pch = 19, bty = \"n\")\n",
        "points(synthetic_data[,1], synthetic_data[,2],\n",
        "      col = viridis::viridis(2, 0.6)[2], pch = 19)"
      ],
      "metadata": {
        "id": "ZsJamoGI0_Wv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}